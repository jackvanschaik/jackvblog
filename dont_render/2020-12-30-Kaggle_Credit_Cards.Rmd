---
title: "Kaggle: Credit Card Customers"
description: "Predicting Credit Card Customer Turnover"
date: 2020-12-30
categories: ["R", "Kaggle"]
tags: ["R Markdown", "R", "Kaggle", "Credit"]
twitterImg: images/clip.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

## Introduction

I've never used Kaggle before, but I know it's popular site for practicing Data Science. I thought it would be fun to download a dataset and give it a go.

I found this [Credit Card customer](https://www.kaggle.com/sakshigoyal7/credit-card-customers) dataset posted by Sakshi Goyal. It's rated gold. I don't know what that means, but it's probably a good thing.

Per the description, the goal is to predict customer attrition. There is noted class imbalance in the outcome variable:

> We have only 16.07% of customers who have churned. Thus, it's a bit difficult to train our model to predict churning customers. 

## Data Cleaning

I downloaded the data, extracted, and saved locally.

```{r}
library(readr)
library(janitor)

credit <- clean_names(read_csv("C:/Users/Jack/Documents/data/kaggle/credit_card_customers/BankChurners.csv"))
```

Some quick checks:

```{r}
dim(credit)
sum(apply(credit, 2, anyNA))
table(credit$attrition_flag)
```
The data description advised ignoring the last two columns, so I will. I also want to transform attrition flag so the outcome = 1, just a personal preference.

```{r}
library(dplyr)

credit %>% 
    mutate(attrition = if_else(attrition_flag == "Attrited Customer", 1, 0)) %>%
    select(attrition, customer_age:avg_utilization_ratio) ->
    credit_2
```

Train test split

```{r}
set.seed(1234)
N <- nrow(credit_2)
I <- sample(N, floor(N*0.70))
cred_train <- credit_2[I,]
cred_test <- credit_2[-I,]
```

We just want to quickly make sure the class imbalance is maintained in the training and test set:

```{r}
table(cred_train$attrition)/nrow(cred_train)
table(cred_test$attrition)/nrow(cred_test)
```
Looks's good

## Logistic Regression

Logistic regression is a natural first choice. We have a binary outcome, categorical and numeric predictors, and n > p. 

#### Individual Predictors

I'll first run a logistic regression model on each individual predictor. A high performing model based on a single predictive could be helpful from a business perspective. 

```{r}
library(pROC)
library(purrr)
library(rlang)

log_reg_test_auc <- function(predictor) {
    reg <- glm(
        formula(paste0("attrition ~ ", predictor)), 
        data = cred_train, 
        family="binomial"
    )
    
    y <- cred_test$attrition
    y_pred <- predict(reg, cred_test, type="response")
    as.numeric(roc(y ~ y_pred)$auc)
}

data.frame(
    predictors = names(cred_train[-1])
) %>%
    mutate(test_auc = unlist(map(predictors, log_reg_test_auc))) %>%
    arrange(desc(test_auc)) %>%
    knitr::kable()
```

So it looks like `total_trans_ct` alone has the best AUC and decent predictive power. Let see how this compares to a logistic regression model with all predictors

#### All Predictors

```{r}
reg <- glm(attrition ~ ., data = cred_train, family="binomial")
y <- cred_test$attrition
y_pred <- predict(reg, cred_test, type="response")
roc(y ~ y_pred)$auc
```

We got a warning there, letting us know we have a rank-deficient fit. This just means we have correlated predictors. Let's identify and remove them.

#### Removing Correlated Predictors

```{r}
rowSums(abs(cor(model.matrix(~., data=cred_train)[,-1])) > 0.95)
```

```{r}
cred_train_2 <- select(cred_train, -avg_open_to_buy)

reg <- glm(attrition ~ ., data = cred_train_2, family="binomial")
y <- cred_test$attrition
y_pred <- predict(reg, cred_test, type="response")
log_roc <- roc(y ~ y_pred)
log_roc$auc
```
There we go, no longer rank-deficient.

#### Performance Evaluation

Let's take a closer look at the model performance.

```{r}
library(ROCR)

perf <- performance(prediction(y_pred, y), "tpr", "fpr")
plot(perf, main="Test ROC Curve for Logistic Regression")
abline(a = 0, b = 1, col = "gray60")
```


```{r}
plot(performance(prediction(y_pred, y), "acc"), main="Testing Accuracy vs Cutoff")
baseline_acc <- max(table(cred_test$attrition)/nrow(cred_test))
abline(h=baseline_acc, col = "gray60")
```

Best accuracy vs baseline

```{r}
baseline_acc
max(unlist(performance(prediction(y_pred, y), "acc")@y.values))
```

## Conclusion

Due to class imbalance, always predicting customer attrition results in a baseline accuracy of 84.5%. Using logistic regression, we can improve accuracy by about 6%. Most of the predictive power seems to come from credit card utilization variables rather than demographic variables. 

While more sophisticated models could probably perform better, this is a good starting point. I'll probably come back to other models later. Logistic regression has the advantage of (somewhat) interpretable coefficients.

## Session Info

```{r}
session_info()
```

