---
title: "Policy Analysis: Twitter vs Facebook on Manipulated Media"
author: "Jack VanSchaik"
date: 2020-11-04
categories: ["Policy"]
tags: ["Policy", "Ethics", "Fake News"]
twitterImg: /images/tidy_2.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

## Policy Analysis: Twitter vs Facebook on Manipulated Media

I will analyze and compare Twitter and Facebook’s synthetic and manipulated media policies. Twitter’s policy is made available on the Rules and Policies section of their website (Twitter, 2020a), while Facebook’s site has two smaller sub policies for “False News” (Facebook, 2020a) and “Manipulated Media” under their Community Guidelines (Facebook, 2020b).

Twitter’s synthetic and manipulated media policy aims to inform users about rules regarding fake and deceptive content posted on the platform and how Twitter may respond to such content. The policy falls under a broader set of rules on the Twitter website that generally establishes how a user may use and engage with the social media site. This particular policy defines three criteria for potentially troubling posts “Is the content synthetic or manipulated?”, “Is the content shared in a deceptive manner?”, and “Is the content likely to impact public safety or cause serious harm?”. If a user posts content that meet some combination of these criteria, the policy states Twitter may label the content, lower its prevalence on the Twitter feed, or remove it entirely. In no way is disinformation banned outright. 

The values espoused by this policy should be considered their broader context. Under the “Twitter Rules” (Twitter, 2020b), Twitter states that its purpose is to “serve the public conversation.” The manipulated media policy falls beneath an “Authenticity” category which elaborates Twitter’s purpose by ensuring “all people can participate in the public conversation freely and safely”. In the policy itself, the only justification provided is Twitter’s imperative to not “cause harm”. Here, “harm” is not given any additional clarification. A bit more subtle is Twitter’s implicit prioritization of violence-inciting disinformation over other fabrications. While Twitter may label any disinformation, the content is only likely to be removed if it has potential to “impact public safety or cause serious harm”. Violence and violent rhetoric are clearly of high importance to Twitter as there are several other seperate policies regarding violence, for example the “Violent threats”, “Violent organizations” and several others. Per this policy, violence elevates disinformation to a category worth removing. This is reflective of western laws about free speech, particularly federal laws about expression in Twitter’s home country, the United States. In particular, the federal government in the United States does not protect speech that incites illegal activity such as violence (Jaeger & Natalie 2019). Conversely, disinformation itself is not inherently illegal.

This policy is written directly to the Twitter user (“You may not”, “you should”, etc..), by the platform itself (“We use”, “we must”, “we are”). Twitter policies are for the most part a contract between the platform and its users. Twitter does not establish the relationship between manipulated media and third-party nonusers. For example, many internet news articles will embed Tweets, and sometimes Twitter content will be displayed on television news. The policy only addressed content as it lives on the platform and does not address upstream or downstream information sinks and sources. In a sense, the policy is reactionary: it does not include provisions for banning consistent manipulated media unless those sources have a Twitter account that violates some other ban-worthy policy.

Facebook has a large set of “Community Standards” that constitute rules what is and is not allowed on their platform. There are several individual policies under these community standards. The purpose of these policies, are outlined as follows in the Community Standards introduction:

> “We recognize how important it is for Facebook to be a place where people feel empowered to communicate, and we take seriously our role in keeping abuse off our service. That’s why we’ve developed a set of Community Standards that outline what is and is not allowed on Facebook.”¬ (Facebook, 2020c)

In other words, the stated goal of Facebook’s policies is to cultivate an environment where people can feel comfortable sharing information. “Abuse” is not defined on the page. I have chosen to focus on Policy 21 (False News) and Policy 22 (Manipulated Media). Both policies fall under a broader category of standards called “Integrity and Authenticity”. For the sake of this analysis, I will treat these two policies as related and compare them to Twitter’s “Synthetic and Manipulated Media” policy. Facebook succinctly states that the goal of “Authenticity” policies is to “[create] a better environment for sharing”. Again, the policies emphasize the user’s freedom to spread content on the platform (in other words, use Facebook). 

The False News policy is rather short but is best considered in the broader context of the Community Standards as well as the content that it links to. For example, the policy links to Facebook Business article that plainly states “false news does not violate our Community Standards” (Lyons 2018) and goes on to explain that false news will only be removed from Facebook if it violates existing community standards. This is just hand waving; Facebook has no policy about False News. The only substantive procedures outlined in Policy 21 are that false news may be given lower prevalence in one’s feed. The extent or exact procedure is not outlined in the policy. Policy 22, regarding manipulated media is a bit more substantive, it provides a specific of regulation for “manipulated media” that Facebook users should not post. Facebook states that they aim to entirely remove this category of media.

In terms of values, Facebook’s Community Guidelines hold the ability to communicate and share content to the utmost importance. Furthermore, Policy 21 upholds the value of volume over veracity of communication. Facebook provides no rigorous ethical justification for the importance of sharing content; one can infer why a company’s chief moral principle aptly aligns with their business model. There are some details in Policy 22 that further nuance the Facebook’s values. While AI derived deep-fake videos are not allowed, content that omits words or changes the order of words in a video is not covered under the policy. Facebook is essentially allowing media manipulation up to a certain level of technological sophistication. The average Facebook user can more easily crop a video on their phone than they can run an AI to create a deep fake. By allowing the more accessible form of media manipulation, Facebook implicitly values the individual user’s capability to spread disinformation.

Throughout the guidelines, the stakeholders of interest are Facebook and its users. This is made explicit in Policy 22. A specific section about “stakeholder engagement” concludes the Community Guidelines by stating that stakeholders are individuals who are impacted the guidelines, which includes every Facebook users (Facebook 2020d). They remind us that this user base consists of over 2.7 billion people and that they cannot possibly engage with every user and therefore use proxies such as civil groups and academics. 

The synthetic and manipulated media policies of Facebook and Twitter share similar structure and presentation, scope, deference to other policies and general lack of defensibility. The key difference between the two is their approach to False New (synthetic media). They also differ in their definition of manipulated media and what guides their response. 

Structurally, the policies are presented on two web pages. The layouts are strikingly similar, and were it not for differing color schemes, it would be difficult distinguish the two. Interestingly, the policies are presented underneath the sites larger domains, twitter.com and facebook.com. If one is logged into Twitter, their account icon displays in the page header. Navigating to one’s Facebook feed is only a few clicks away. This indicates to the platform’s users that they are the intended audience. This seems subtle at first, but its analogous to a library printing policy on the inner cover of all its circulating books. Websites extend policy writing beyond written word to an amalgamation of text, code, and web design. In this case, both Twitter and Facebook intentionally utilize user data to contextualize policy in the personal experience the platforms are famous for.

Both policies are presented as sub policies of larger policy collections; for Facebook, it is under “Community Standards, for Twitter it’s under “General Guidelines and policies”. Comparing the entirety of the two policy collections is out of the scope here, but at first glance they are somewhat similar: there are many other smaller policies about regulations and freedoms of the platform users. The false news and manipulated media policies do have a similar scope, as the policy is dictated by the ambiguous “we” of the platform to the user”. Twitter explicitly uses the word “you” while Facebook does not. Instead it has a header that says “DO NOT” followed by a definition of manipulated media. Both platforms, to a slightly varying extent, include the users and themselves as responsible parties, but make little mention of other responsible parties outside of Facebook’s somewhat vague “Stakeholder Engagement” section. It seems that the policy is meant to be read by the users but implicitly known by the platform in either case. As far as policy adherence goes, authority seems to lie solely with the social media sites. They take the role of judge, jury, and executioner as far as enforcement goes. One structural distinction is that every Facebook policy is prepended by a “Rationale” section, whereas Twitter’s rationale is alluded to in the policy text. 

Facebook and Twitter differ substantially in the procedures of their manipulated media policies. Again, Facebook has two separate sub-policies for False News and Manipulated Media. Facebook allows false news to be posted, so the policy is essentially a list of ways they are attempting to “reduce the spread of false news”. This is different from Twitter that has a single “synthetic and manipulated media” policy. Twitter does not make distinction between synthetic media that is fake news, but unlike Facebook it does not explicitly express that there is no rule against it. Twitter offers a table that details which type of deceptive content could warrant specific reactions. Facebook on the other hand draws an exact line of allowable media manipulations in the “Do Not Post” section of Policy 22. Both Facebook and Twitter give special attention to violent content, and both defer to more general policies. 

These policies are not perfect, the main flaw in both being the lack of ethical foundation or rationale for the procedures. Both social media platforms, especially Facebook, base their policies in the desire to cultivate an environment of ample conversation. Facebook and Twitter fail to establish why this is good. Consider this passage from the introduction to Facebook’s community standards. 

> “We recognize how important it is for Facebook to be a place where people feel empowered to communicate, and we take seriously our role in keeping abuse off our service.” (Facebook, 2020c)

If Facebook takes abuse seriously, perhaps they could take the trouble to define it, but they do not. This makes justifying procedures harder down the line. It is only possible to prevent abuse if you know what it is. Furthermore, in policy 22 Facebook states “There is also a fine line between false news and satire or opinion”. One would think if the line is that fine, they could easily draw it but, yet again, they do not. Facebook’s policy needs a more precise definition of abuse. Distinctions that are supposedly clear may not be obvious to all and should be made explicit. 

Another issue is Twitter’s procedural incertitude. A good policy is one that provides exact procedures in response to exact events. In Twitter’s policy, any possible content procedures are prepended by either “may” or “likely”. This uncertainty equates to opacity about decisions being made behind these scenes. Content can possibly be removed, but the final decision made is the result of an internal process that may or may not follow a consistent procedure. Twitter could improve this by providing historical data of why flagged content got removed; thereby giving more meaning to terms like “may” or “likely”. 

Exacting definitions can be problematic too. As mentioned before, Facebook draws a very distinct line for acceptable types of manipulated media. The allowance of more accessible means of media manipulation in a way invites it. If the more sophisticated methods are banned, why bother using them? Facebook also concludes their manipulated media policy by mentioning it would be impossible to get feedback from all their 2+ billion users, so they use community proxies instead. How are users supposed to take a policy seriously when on an individual level their input is not even considered? It is also just an outright lie. Facebook collects massive amounts of data on every single one of its billions of users with every interaction and has a whole infrastructure for researching this data. It would be easy to engage Facebook users with policy on an individual level and collect feedback. Furthermore, these platforms have other stakeholders besides users and themselves; its neigh impossible for any human on Earth to escape the influence of these platforms. About 2% of humans who have ever lived have had a Facebook account at some point (Stephenson 2012). The policies could be improved by explicitly addressing how the distribution of synthetic media effects the rest of the world.

A structural weakness in Twitter’s policy is the apparent lack of version control or temporal context. Twitter could improve this by following Facebook’s example and including a link to “Recent Updates” in a clearly visible location on the site. Both could benefit from exact publishing dates on each policy page instead of just a publishing year. 

In conclusion, in their role as social media platforms, Facebook and Twitter market in the communication of information. Every single one of their policies is therefore an information policy that can affect billions of people. Fake and synthetic media is a rising problems that these platforms attempt to address with their own distinct policies. Currently, these policies are written as instructions from the all-powerful platform, to the individual user, along with warnings about what how they might exercise their complete power to regulate content. While these policies define clear procedures in some places, they are ambiguous and lack ethical foundation in others. These platforms reach everywhere in the world; we would be better served by more rigorous and morally justified policies. 

## References

Jaeger, P. T., & Natalie, G. T. (2019). Types of laws, policies, and regulations impacting information: Access, rights, and responsibilities. In Foundations of information policy (pp. 105–122). Chicago, IL: ALA Neal-Schuman.

Facebook. (2020a). False News. [https://www.facebook.com/communitystandards/false_news](https://www.facebook.com/communitystandards/false_news).

Facebook. (2020b). Manipulated Media. [https://www.facebook.com/communitystandards/manipulated_media](https://www.facebook.com/communitystandards/manipulated_media). 

Facebook. (2020c). Community Standards. [https://www.facebook.com/communitystandards/introduction](https://www.facebook.com/communitystandards/introduction) 

Facebook. (2020d). Stakeholder Engagement. [https://www.facebook.com/communitystandards/stakeholder_engagement](https://www.facebook.com/communitystandards/stakeholder_engagement)

Lyons, T. (2018). Hard Questions: What’s Facebook’s Strategy for Stopping False News?. Facebook Business. [https://about.fb.com/news/2018/05/hard-questions-false-news/](https://about.fb.com/news/2018/05/hard-questions-false-news/) 

Stephenson, W. (2012). Do the dead outnumber the living? BBC news. [https://www.bbc.com/news/magazine-16870579](https://www.bbc.com/news/magazine-16870579) 

Twitter. (2020a). Synthetic and manipulated media policy. [https://help.twitter.com/en/rules-and-policies/manipulated-media](https://help.twitter.com/en/rules-and-policies/manipulated-media)

Twitter. (2020b). Twitter rules. [https://help.twitter.com/en/rules-and-policies/twitter-rules](https://help.twitter.com/en/rules-and-policies/twitter-rules) 

