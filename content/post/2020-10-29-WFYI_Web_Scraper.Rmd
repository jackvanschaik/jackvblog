---
title: "WFYI WEb Scraper"
author: "Jack VanSchaik"
description: "Scrape Web Articles from WFYI via an search phrase"
date: 2020-10-29
categories: ["R"]
tags: ["R Markdown", "R"]
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[WFYI] is Indianapolis' Public Radio station. They offer some text news articles online. I was interested in Indianapolis news data for another project so I built this scraper. It could come in handy if you're interested in local news data, or just want to see how a news web scraper works. I've built several and they all work in a similar manner. I'll probably post them later!

Here's the R code:

## Web Scraper Code

```{r}
library(tidyverse) # For easy data manipulation
library(rvest) # Used to actual web scraping

# Get all the articles links on a particular search results page
get_page_links <- function(q, pg = 1) {
    url <- sprintf("https://www.wfyi.org/search?q=%s&pg=%s", q, pg)
    res <- read_html(url)
    res %>%
        html_nodes(xpath='//*[@id="maincontent"]/div/*/div[2]/a') %>%
        html_attrs %>%
        unlist %>%
        as.character ->
        links
    
    links
}


# Get the total number of pages of search results, then iterate through each 
# page and get all the links on each page
get_all_pages_links <- function(q) {
    url <- sprintf("https://www.wfyi.org/search?q=%s", q)
    res <- read_html(url)
    res %>% 
        html_nodes(xpath='//*[@id="maincontent"]/div/p/text()') %>%
        as.character %>% 
        str_extract_all("[[0-9]]+ results") %>%
        str_extract("[[0-9]]+") %>%
        as.numeric ->
        n_results
    
    n_pages <- ceiling(n_results/10)
    
    L <- lapply(1:n_pages, function(j) get_page_links(q=q, pg=j))
    do.call(c, L)
}

# Get the text of an article at a given url
get_article_text <- function(url) {
    html <- read_html(url)
    content <- html_text(html_node(html, xpath='//*[@id="maincontent"]/div/div[1]/div[2]'))
    dt <- html_text(html_node(html, xpath='//*[@id="maincontent"]/div/div[1]/div[1]/h5/text()[2]'))
    dt_2 <- substr(dt, 3, nchar(dt))
    date <- as.Date(dt_2, format="%n%B%n%d,%n%Y%n")
    data.frame(date = date, content=content)
}

# Combine the above functions to get the text of every article 
wfyi_scrape <- function(q) {
    all_page_links <- get_all_pages_links(URLencode(q))
    L <- lapply(all_page_links, get_article_text)
    all_articles <- do.call(bind_rows, L)
    filter(all_articles, !is.na(date) & !is.na(content))
}
```

## Quick Example

Now that the functions are in place, scraping articles for a particular search phrase is quite easy. Keep in mind, it may take a few minutes to get all the web pages and parse them.

```{r}
results <- wfyi_scrape("Pete Buttigieg")
DT::datatable(mutate(results, content=paste0(substr(content, 1, 100), "...")))
```

Happy scraping!