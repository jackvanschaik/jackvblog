---
title: "Kaggle: A Low Usability Dataset"
description: "Working with a difficult Kaggle Dataset"
date: 2021-01-03
categories: ["R"]
tags: ["R", "R markdown", "Kaggle"]
twitterImg: images/clip.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

## Introduction

Roughly 70% of Statistics, Data Analysis, Data Science, etc is data cleaning. Once your data is ready for analysis, many models are a single line of code away.

Working with messy data is a valuable skill. No dataset is perfectly clean. Even you're lucky enough to be handed "clean" data, you still need to understand how the cleaning process may have impacted and biased the data, and therefore your results.

In addition to user ratingsKaggle has a "usability" associated with each dataset. It seems that usability reflects the completeness of a particular dataset's documentation. While many top rated datasets have a high usability score, (9 to 10 range), I thought it would be a useful exercise to analyze a dataset with a low usability score. 

I found this [Loan Prediction Problem Dataset](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset) posted by Debdatta Chatterjee. It has a decent rating and a seemingly good number of upvotes, but a very low usability score of 1.8.

I hope to showcase some technical data preparation techniques, as well as explain the intuition and though process for why certain cleaning steps are taken.

## Importing the Data

The files are csv, so I'll use `readr::read_csv`. There's a train and a test dataset with seemingly meaningless file names. 

```{r}
library(readr)

lp_test <- read_csv("C:/Users/Jack/Documents/data/kaggle/loan_prediction/test_Y3wMUE5_7gLdaTN.csv")
lp_train <- read_csv("C:/Users/Jack/Documents/data/kaggle/loan_prediction/train_u6lujuX_CVtuZ9i.csv")
```

The data loaded without any problems. A personal preference is to transform column names to lower snake case with janitor:

```{r}
library(janitor)

lp_test <- clean_names(lp_test)
lp_train <- clean_names(lp_train)
```

I noticed a difference in the number of columns in the Environment window (I'm using Rstudio).

```{r}
setdiff(names(lp_train), names(lp_test))
```

Well it looks like the "test" dataset is missing the `loan_status` column. The name of the dataset is "Loan Prediction", and we have no other documentation, so my best guess is that this is our outcome of interest. Meaning our test dataset doesn't have our outcome variable. Does that mean it's useless? No, but I wouldn't quite call it test data...

## Exploring the Columns

Let's get an overview of each column

```{r}
library(dplyr)

data_check <- function(X) {
    tibble(
        col_name=names(X),
        class=t(summarise(X, across(everything(), class))),
        missing=t(summarise(X, across(everything(),  ~ sum(is.na(.x)) )))
    )
}

knitr::kable(data_check(lp_train))
```

So there are a few variables with missing values. 

## Adjusting Columns

If we look closely at the values of `loan_amount_term`:

```{r}
count(lp_train, loan_amount_term)
```

These seem to be numbers of days, and take on only a few values. Due to limited documentation, there's no way to know for certain if we should treat this as numeric or not. My intuition is that the unit here is months (360 = 30 year mortage), and the loan applicants probably get a fixed set of term amounts to choose from. Another thing to consider is if order matters. If the ">" or "<" symbols are meaningless for a variable, it probably shouldn't be treated as numeric. For `loan_term_amount`, you could make a case either way. 

Based on how I plan to approach missing values for this variables, I'm going to treat it as categorical. I think there are only a few fixed values, so this is justifiable.

```{r}
lp_train <- mutate(lp_train, loan_amount_term  = as.character(loan_amount_term))
```

Also, credit history is surely and categorical variable here:

```{r}
knitr::kable(count(lp_train, credit_history))
lp_train <- mutate(lp_train, credit_history  = as.character(credit_history))
```


## Approaching Missingness

The naive thing to do would be to throw out any rows with NAs:

```{r}
sum(apply(is.na(lp_train), 1, any))
sum(apply(is.na(lp_train), 1, any))/nrow(lp_train)
```

But that would amount to throwing out 134 rows. Which is 22% of an already small training set (n=614).

The key here is that we have next to no documentation, so we can't make any assumptions about why the data is missing, or what missing data means. The fact that the data is missing itself may be important for our prediction. We can account for this by adding a "Missing" level to categorical variables. For numerical data, it's good practice to add a missingness indicator.

```{r}
library(tidyr)

data_transform_1 <- function(data_1) {
    data_1 %>%
        mutate(across(where(is.character), replace_na, "Missing")) ->
        data_2
    
    data_2 %>%
        select(where(is.numeric)) %>%
        mutate(across(everything(), ~ if_else(is.na(.x), 1, 0))) %>%
        `names<-`(., paste0(names(.), "_missing")) %>%
        cbind(data_2, .) ->
        data_3
    
    data_3
}


lp_train_2 <- data_transform_1(lp_train)
lp_test_2 <- data_transform_1(lp_test)
```

That still leaves us with missing numeric values. Again, due to limited documentation, we have to make a few educated guesses here. Predicting loan rejection seems more likely to be a business use case than an academic one. We can assume that predictive power is more important than inference here. 

## Imputation with Random Forests

Why not use Random Forests to impute our missing numeric values? We can use the marginal distribution of nonmissing variables to predict the missing values, while making use of the (seemingly useless) test dataset to see how well our imputation performs. 

We'll set up a function to do this one variable at a time.

```{r}
library(randomForest)
library(rlang)

impute_rf <- function(target_data, formula, test_data) {
    # Subset the training data to what needs imputed
    lhs <- f_lhs(formula)
    rhs <- f_rhs(formula)
    
    needs_impute <- is.na(target_data[[lhs]])
    target_train <- target_data[!needs_impute,]
    target_imp <- target_data[needs_impute,]

    # Train the random forest with nonmissing values
    Rf <- randomForest(formula, data=target_train, ntree = 200)
    
    # Calculate R2 against complete cases in the test data
    test_complete <- drop_na(test_data)
    y_pred <- predict(Rf, test_complete)
    y <- test_complete[[lhs]]
    tss <- sum((y - mean(y))^2)
    rss <- sum((y - y_pred)^2)
    r2 <- 1 - rss/tss
    cat("Test R2 for Imputation:", r2, fill=TRUE)
    
    # Impute the missing values and return
    target_data[needs_impute, as.character(lhs)] <- predict(Rf, target_imp)
    target_data
}
```


We can now impute our missing values:

```{r}
set.seed(1234)

lp_train_2 %>%
    impute_rf(
        loan_amount ~ married + dependents + education + self_employed +
            applicant_income + coapplicant_income + property_area + loan_amount_term,
        lp_test_2
    ) ->
    lp_test_3

knitr::kable(data_check(lp_test_3))
```


## Conclusion

There you have it. The data is now analysis ready. We started with no documentation, used a combination of educated guesses and best practices, and finally created a complete, analysis ready dataset. 

It's very important that data preparation is documented and explained in a clear and cohesive manner. Data prep should be reproducible in both academia and research. Relevant details about imputation (such as the $R^2$ value above), should be shared in a methods section or presented to relevant stakeholders. R markdown is a great way to implement a reproducible, well documented, data cleaning process. 

While a "Usability" score can help Kaggle users find working practice datasets, real world data doesn't come with a usability score. Often, a lot of work goes into preparing data for analysis. Understanding data preparation and cleaning is for necessary for interpreting results. If you are new to data analysis and are using Kaggle for practice that's great! I'd encourage you to try out some low "usability" datasets to practice your data wrangling skills.

## Data Ethics Postscript

Algorithmic prediction of loan approval has been problematic. I recommend Cathy O'Neil's [Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815)-- Chapter 8 is particularly relevant here.

Part of working with data, especially messy data, is realizing that it exists as part of a much larger system. It is never raw, usually biased, and mostly likely will lead to biased results. Datasets such as this one, that are completely devoid of documentation, should be handled with extreme caution.

## Session Info

```{r}
session_info()
```

