---
title: "Analysis: Stop The Steal Hashtag"
author: "Jack VanSchaik"
date: 2020-11-06
categories: ["R"]
tags: ["Election", "R", "Twitter"]
twitterImg: /images/sts_1.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

## What is #StopTheSteal

The "StopTheSteal" hashtag has had a rampant uptake in popularity since the US election on 11/3. It's now 11/6 and votes are still being counted, mostly due to the huge increase in mail-in voting. There is no substantive evidence of voter fraud, but partisan twitter is now rife with election conspiracy theories.

I've done a quick analysis of the conversation around #StopTheSteal as its developing. The discourse will likely change within hours, so this is snapshot of 11/6 at around 6 pm EST.

## Analysis

I've included my analysis with source code and some resulting data.

#### Get Recent Tweets

I gathered the data with the following code, and saved to my hard drive for reproducibility sake:

```{r, eval=FALSE}
library(rtweet)
library(tidyverse)
rt <- search_tweets("#StopTheSteal", n = 18000, include_rts = TRUE)
rt <- select(rt, created_at, status_id, text)
fst::write_fst(rt, "C:/Users/Jack/Documents/data/twitter/sts_tweets.fst")
```

I loaded back into R and created a corpus with quanteda.

```{r, results="hide", message=FALSE}
library(tidyverse)
library(quanteda)
library(lubridate)
```

```{r}
rt <- fst::read_fst("C:/Users/Jack/Documents/data/twitter/sts_tweets.fst")
rt <- mutate(rt, created_at = with_tz(created_at, "EST"))
tweet_cor <- corpus(rt, docid_field = "status_id", text_field = "text")
```

#### Visualizing The Frequency of Recent Tweets

The tweets I gathered go back to this morning. They average about 90 tweets every 5 minutes:

```{r}
ggplot(rt, aes(x=created_at)) + 
    geom_histogram(stat="bin", binwidth=5*60, fill="#d6093f") + 
    labs(title="Frequency of recent #StopTheSteal tweets", x="Time, 11/6 EST", y="Tweet Count") +
    theme_minimal()
```

#### Tokenize

For this analysis, I'll be looking at the most common word combinations of 3 to 5 words in tweets containing the hashtag. To do this, we'll first need to separate words (tokenize), and clean the text.

```{r}
tweet_cor %>%
    tokens(remove_punct = TRUE, remove_symbols = TRUE, remove_url=TRUE) %>%
    tokens_tolower() %>%
    tokens_keep(pattern="[A-z0-9@#_]+", valuetype = "regex") %>%
    tokens_ngrams(n = 3:5) ->
    tok_ngram
```

#### Get Top n-grams

Now, we'll get the top 100 most frequent word combinations. I've made a table of these so you can see for yourself.

```{r}
tok_ngram %>%
    dfm %>%
    textstat_frequency %>%
    head(n=100) ->
    top100

DT::datatable(top100)
```

## Discussion

The most frequent word combinations all pointed to a phrase at the end of the Declaration of Independence:

> "We Mutually Pledge To Each Other Our Lives, Our Fortunes And Our Sacred Honor"

This has become the mantra of the new #StopTheSteal movement, and a call to action for widespread protests at state every state capitol, tomorrow, Nov 7th at noon.

There's no denying that the #StopTheSteal movement, founded on a false premise or not, has spread rapidly over the internet. Most tweets are more call to action than argument. The incorporation of the Declaration of Independence into the messaging exemplifies familiar right wing fervor. Any upcoming protests should be taken seriously, and vehement action should be expected and prepared for.

Here's 10 of the most recent tweets that repeat the aforementioned phrase:

```{r}
rt %>%
    filter(str_detect(tolower(text), fixed("free and fair"))) %>%
    arrange(desc(created_at)) %>%
    head(n=10) %>%
    knitr::kable()
```

## R Session Info

```{r}
session_info()
```

