---
title: "WFYI Web Scraper"
author: "Jack VanSchaik"
description: "Scrape Web Articles from WFYI via an search phrase"
date: 2020-10-29
categories: ["R"]
tags: ["R Markdown", "R"]
twitterImg: /images/pete.png
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>


<p><a href="https://www.wfyi.org/">WFYI</a> is Indianapolis’ Public Radio station. They offer some text news articles online. I was interested in Indianapolis news data for another project so I built this scraper. It could come in handy if you’re interested in local news data, or just want to see how a news web scraper works. I’ve built several and they all work in a similar manner. I’ll probably post them later!</p>
<p>Here’s the R code:</p>
<div id="web-scraper-code" class="section level2">
<h2>Web Scraper Code</h2>
<pre class="r"><code>library(tidyverse) # For easy data manipulation
library(rvest) # Used to do the actual web scraping</code></pre>
<pre class="r"><code># Get all the articles links on a particular search results page
get_page_links &lt;- function(q, pg = 1) {
    url &lt;- sprintf(&quot;https://www.wfyi.org/search?q=%s&amp;pg=%s&quot;, q, pg)
    res &lt;- read_html(url)
    res %&gt;%
        html_nodes(xpath=&#39;//*[@id=&quot;maincontent&quot;]/div/*/div[2]/a&#39;) %&gt;%
        html_attrs %&gt;%
        unlist %&gt;%
        as.character -&gt;
        links
    
    links
}


# Get the total number of pages of search results, then iterate through each 
# page and get all the links on each page
get_all_pages_links &lt;- function(q) {
    url &lt;- sprintf(&quot;https://www.wfyi.org/search?q=%s&quot;, q)
    res &lt;- read_html(url)
    res %&gt;% 
        html_nodes(xpath=&#39;//*[@id=&quot;maincontent&quot;]/div/p/text()&#39;) %&gt;%
        as.character %&gt;% 
        str_extract_all(&quot;[[0-9]]+ results&quot;) %&gt;%
        str_extract(&quot;[[0-9]]+&quot;) %&gt;%
        as.numeric -&gt;
        n_results
    
    n_pages &lt;- ceiling(n_results/10)
    
    L &lt;- lapply(1:n_pages, function(j) get_page_links(q=q, pg=j))
    do.call(c, L)
}

# Get the text of an article at a given url
get_article_text &lt;- function(url) {
    html &lt;- read_html(url)
    content &lt;- html_text(html_node(html, xpath=&#39;//*[@id=&quot;maincontent&quot;]/div/div[1]/div[2]&#39;))
    dt &lt;- html_text(html_node(html, xpath=&#39;//*[@id=&quot;maincontent&quot;]/div/div[1]/div[1]/h5/text()[2]&#39;))
    dt_2 &lt;- substr(dt, 3, nchar(dt))
    date &lt;- as.Date(dt_2, format=&quot;%n%B%n%d,%n%Y%n&quot;)
    data.frame(date = date, content=content)
}

# Combine the above functions to get the text of every article 
wfyi_scrape &lt;- function(q) {
    all_page_links &lt;- get_all_pages_links(URLencode(q))
    L &lt;- lapply(all_page_links, get_article_text)
    all_articles &lt;- do.call(bind_rows, L)
    filter(all_articles, !is.na(date) &amp; !is.na(content))
}</code></pre>
</div>
<div id="quick-example" class="section level2">
<h2>Quick Example</h2>
<p>Now that the functions are in place, scraping articles for a particular search phrase is quite easy. Keep in mind, it may take a few minutes to get all the web pages and parse them.</p>
<pre class="r"><code>results &lt;- wfyi_scrape(&quot;Pete Buttigieg&quot;)
# Print in a nice data table and limit contents to 100 character
DT::datatable(mutate(results, content=paste0(substr(content, 1, 100), &quot;...&quot;)))</code></pre>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["1","2","3","4","5"],["2020-03-02","2019-07-08","2019-02-10","2018-06-15","2019-11-05"],["Former South Bend Mayor Pete Buttigieg is suspending his campaign for president.Justin Hicks/IPB New...","South Bend Mayor Pete Buttigieg gives an update to the common council on his administration's effort...","South Bend Mayor Pete Buttigieg talks to WNDU's Maureen McFadden at a launch event for his new book ...","In a Saturday, Feb. 25, 2017 photo, South Bend Mayor Pete Buttigieg addresses the Democratic Nationa...","James Mueller hugs a supporter at Corby's Irish Pub after winning the South Bend mayoral election.Ph..."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>date<\/th>\n      <th>content<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>
<p>Happy scraping!</p>
</div>
